variables:
  # Used in GitLab rules
  DEPLOYMENT_DEFAULT_BRANCH: "develop"

# FIXME revisit whenever GitLab gets their act together and makes it easier
# to avoid doubled pipelines…
workflow:
  rules:
    - if: '$CI_EXTERNAL_PULL_REQUEST_IID'
    - if: '$CI_COMMIT_BRANCH == $DEPLOYMENT_DEFAULT_BRANCH'

# This prepares the environment for the rest of the pipeline.  Could have
# been a simple function in a CI environment with a proper DSL ¯\_(ツ)_/¯
#
# Needed as jobs can't inherit from this *and* augment the rules
setup_environment:
  variables:
    # We will do this ourselves, will be on GPFS
    GIT_STRATEGY: none
    # All the variables below should be added to `deployment.env` to build
    # the basis for further stages without relying to much on the
    # `variables` key of GitLab
    #
    # Global default, as long as we have a 9.x.x, environment-modules picks
    # the wrong one as default :(
    DEFAULT_GCC_VERSION: "12.2.0"
    # Used by the Intel compilers
    LEGACY_GCC_VERSION: "11.3.0"
    # Used for cloning Spack
    DEPLOYMENT_BRANCH: "$DEPLOYMENT_DEFAULT_BRANCH"
    # Proprietary sources
    DEPLOYMENT_DATA: "/gpfs/bbp.cscs.ch/ssd/apps/hpc/download"
    # This date should increment for new deployments — about once a year
    DEPLOYMENT_DATE: "2023-11-27"
    # Global base directory
    DEPLOYMENT_BASE: "/gpfs/bbp.cscs.ch/ssd/apps/bsd"
    # Global module archive
    MODULE_ROOT: "$DEPLOYMENT_BASE/modules"
    # Previous deployment modules
    OLD_DEPLOYMENT_MODULES: "/gpfs/bbp.cscs.ch/ssd/apps/hpc/jenkins/modules/all/archive"
    # All directories below should change for pull requests (replacing the
    # date)
    DEPLOYMENT_ROOT: "$DEPLOYMENT_BASE/$DEPLOYMENT_DATE"
    DEPLOYMENT_ROOT_SUFFIX: ""
    DEPLOYMENT_UPSTREAM: "$DEPLOYMENT_BASE/$DEPLOYMENT_DATE"
    DEPLOYMENT_PROPRIETARY_MIRROR: "$DEPLOYMENT_BASE/$DEPLOYMENT_DATE/mirror/proprietary"
    # Artifacts to be passed back from the child pipeline: needs GitLab
    # fix/new feature
    #
    # Keep only the latest for the deployment proper.  See below for PR
    # specialization
    DEPLOYMENT_ARTIFACTS: "$DEPLOYMENT_ROOT/artifacts"
  rules:
    - if: '$CI_EXTERNAL_PULL_REQUEST_IID'
      variables:
        DEPLOYMENT_ARTIFACTS: "$DEPLOYMENT_ROOT/artifacts/$CI_PIPELINE_IID"
        DEPLOYMENT_BRANCH: "$CI_EXTERNAL_PULL_REQUEST_SOURCE_BRANCH_NAME"
        DEPLOYMENT_ROOT: "$DEPLOYMENT_BASE/pulls/$CI_EXTERNAL_PULL_REQUEST_IID"
        DEPLOYMENT_ROOT_SUFFIX: "pulls/$CI_EXTERNAL_PULL_REQUEST_IID"
        DEPLOYMENT_PROPRIETARY_MIRROR: "$DEPLOYMENT_BASE/pulls/$CI_EXTERNAL_PULL_REQUEST_IID/mirror/proprietary"
        # Somewhat awkward, but "$DEPLOYMENT_ROOT/modules" is already used.
        MODULE_ROOT: "$DEPLOYMENT_ROOT/config/modules"
      when: always
    - when: always
  script:
    # The `rules` keyword seems to not inherit variables specified via a
    # dotenv ⇒ do things manually once more
    - echo "DEFAULT_GCC_VERSION=$DEFAULT_GCC_VERSION" > deployment.env
    - echo "LEGACY_GCC_VERSION=$LEGACY_GCC_VERSION" >> deployment.env
    - echo "DEPLOYMENT_ARTIFACTS=$DEPLOYMENT_ARTIFACTS" >> deployment.env
    - echo "DEPLOYMENT_BASE=$DEPLOYMENT_BASE" >> deployment.env
    - echo "DEPLOYMENT_BRANCH=$DEPLOYMENT_BRANCH" >> deployment.env
    - echo "DEPLOYMENT_DATA=$DEPLOYMENT_DATA" >> deployment.env
    - echo "DEPLOYMENT_DATE=$DEPLOYMENT_DATE" >> deployment.env
    - echo "DEPLOYMENT_PROPRIETARY_MIRROR=$DEPLOYMENT_PROPRIETARY_MIRROR" >> deployment.env
    - echo "DEPLOYMENT_ROOT=$DEPLOYMENT_ROOT" >> deployment.env
    - echo "DEPLOYMENT_ROOT_SUFFIX=$DEPLOYMENT_ROOT_SUFFIX" >> deployment.env
    - echo "DEPLOYMENT_UPSTREAM=$DEPLOYMENT_UPSTREAM" >> deployment.env
    - echo "MODULE_ROOT=$MODULE_ROOT" >> deployment.env
    - echo "OLD_DEPLOYMENT_MODULES=$OLD_DEPLOYMENT_MODULES" >> deployment.env
    # Needed to nest `srun` commands within SLURM
    - echo "SLURM_OVERLAP=1" >> deployment.env
  artifacts:
    when: always
    paths: [deployment.env]
    reports:
      dotenv: deployment.env

# GitLab has no scheduling concept on the pipeline level, see
# https://gitlab.com/gitlab-org/gitlab/-/issues/16548
#
# This is just a workaround to at least guarantee that no two (now child)
# pipelines run at the same time. SNAFU
atomic_build:
  needs: [setup_environment]
  rules:
    # If the NEURON recipe has been modified then trigger the NEURON CI against
    # the PR deployment directory.
    - changes: [bluebrain/repo-patches/packages/neuron/package.py]
      variables:
        TRIGGER_CHILD_NEURON_PIPELINE: "1"
    - when: always
  trigger:
    include: bluebrain/deployment/gitlab-ci.yml
    strategy: depend
  variables:
    # Used to store artifacts in the right directory, grab dotenv
    PARENT_PIPELINE_ID: $CI_PIPELINE_ID
    # For feedback stuff, will be empty if not in a PR
    GITHUB_PULL_REQUEST_ID: $CI_EXTERNAL_PULL_REQUEST_IID
    GITHUB_REPOSITORY: $CI_EXTERNAL_PULL_REQUEST_TARGET_REPOSITORY
    GITHUB_API_USER_OVERRIDE: $GITHUB_API_USER_FOR_GITLAB_LOG_UPLOAD
    GITHUB_API_KEY_OVERRIDE: $GITHUB_API_KEY_FOR_GITLAB_LOG_UPLOAD
  resource_group: BlueBrain.GPFS.$CI_EXTERNAL_PULL_REQUEST_IID

forward_tests:
  needs: [setup_environment, atomic_build]
  tags: [bb5_map]
  when: always
  script:
    - rsync -av "$DEPLOYMENT_ARTIFACTS/" .
    - if grep -q -m1 -l -R '<failure' "$DEPLOYMENT_ARTIFACTS"; then exit 1; fi
  artifacts:
    when: always
    paths:
      - spack_tests
    reports:
      junit: spack_tests/*/*.xml

# Trigger basic container builds
update_spacktainerizer:
  needs: []
  # Really only want to run this when builds are active: exclude running
  # on PRs, just merges and schedules.
  rules:
    - if: $CI_EXTERNAL_PULL_REQUEST_IID
      when: never
    - when: always
  trigger:
    project: hpc/spacktainerizer
    strategy: depend

# Trigger build cache population
update_build_cache:
  needs: [update_spacktainerizer]
  # Really only want to run this when builds are active: exclude running
  # on PRs, just merges and schedules.
  rules:
    - if: $CI_EXTERNAL_PULL_REQUEST_IID
      when: never
    - when: always
  trigger:
    project: hpc/spack-cacher
    strategy: depend

# Trigger building containers equivalent to modules
update_containers:
  needs: [update_build_cache]
  # Really only want to run this when builds are active: exclude running
  # on PRs, just merges and schedules.
  rules:
    - if: $CI_EXTERNAL_PULL_REQUEST_IID
      when: never
    - when: always
  trigger:
    project: hpc/spackah

# PRs run as a different user, after merging on `develop`, we need to trigger the cleanup
# project that always runs with the PR builder user
clean_merged_prs:
  needs: []
  # Really only want to run this a few times a day at most: exclude running
  # on PRs, just merges and schedules.
  rules:
    - if: $CI_EXTERNAL_PULL_REQUEST_IID
      when: never
    - when: always
  trigger:
    project: hpc/spack-pr-cleaner
  variables:
    DEPLOYMENT_BASE: $DEPLOYMENT_BASE
    GITHUB_PULL_REQUEST_ID: "merged"


# some changes require a human to be notified. We'll do this through JIRA tickets
notifications:
  needs: [atomic_build]
  image: python:3.11-bullseye
  rules:
    - if: '$CI_COMMIT_BRANCH == $DEPLOYMENT_DEFAULT_BRANCH'
      changes:
        - var/spack/repos/builtin/packages/apptainer/*
        - var/spack/repos/builtin/packages/singularity/*
        - var/spack/repos/builtin/packages/singularityce/*
  script:
    - apt-get update
    - apt-get install -y jq wget
    - echo "Finding ID of the atomic build"
    - ATOMIC_BUILD_ID=$(wget --header "Content-Type:application/json" --header "PRIVATE-TOKEN:${GITLAB_API_KEY}" -O- "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/pipelines/${CI_PIPELINE_ID}/bridges" | jq '.[] | select(.name == "atomic_build") | .downstream_pipeline.id')
    - echo "Atomic build pipeline ID is ${ATOMIC_BUILD_ID}"
    - set +e
    - wget --header "Content-Type:application/json" --header "PRIVATE-TOKEN:${GITLAB_API_KEY}" -O- "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/pipelines/${ATOMIC_BUILD_ID}/test_report"
    - |
      wget --header "Content-Type:application/json" --header "PRIVATE-TOKEN:${GITLAB_API_KEY}" -O- "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/pipelines/${ATOMIC_BUILD_ID}/test_report" | jq -e '.test_suites[] | select(.name == "externals") .test_cases[] | select(.status != "skipped" and (.classname == "singularity" or .classname == "singularityce" or .classname == "apptainer"))'
      if [[ $? -ne 0 ]]
      then
        echo "No test found - no need to warn anyone."
      else
        echo "Relevant test found - time to create a HELP ticket"
        pip install -r bluebrain/notifications/requirements.txt
        python bluebrain/notifications/jira_ticket.py --project HELP --summary "Please run the spack permissions fix scripts" --description-file bluebrain/notifications/apptainer_singularity.txt
      fi
